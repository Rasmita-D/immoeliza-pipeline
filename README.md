## :classical_building: Description

This project explores using airflow to automate the scraping, analysis and preprocessing of house data to create a machine learning model that predicts house prices.

API: https://immoeliza-pipeline-57k8.onrender.com/

##	:building_construction: Repo Structure
```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ api
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”œâ”€â”€ app.py
â”‚Â Â  â”œâ”€â”€ predict.py
â”‚Â Â  â”œâ”€â”€ preprocess.py
â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â”œâ”€â”€ utils
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ kitchen_ordinal.pkl
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ one_hot.pkl
â”‚Â Â  â”‚Â Â  â””â”€â”€ state_building_ordinal.pkl
â”‚Â Â  â””â”€â”€ xgboost.pkl
â”œâ”€â”€ dags
â”‚Â Â  â”œâ”€â”€ api.py
â”‚Â Â  â”œâ”€â”€ data_analysis_graphs_dag.py
â”‚Â Â  â”œâ”€â”€ data_cleaning_training_dag.py
â”‚Â Â  â”œâ”€â”€ house_data_scraping_dag.py
â”‚Â Â  â””â”€â”€ training_models.py
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ houses_data_{date}.csv
â”‚Â Â  â”œâ”€â”€ houses_urls_{date}.txt
â”‚Â Â  â”œâ”€â”€ test-{date}.csv
â”‚Â Â  â””â”€â”€ train-{date}.csv
â”œâ”€â”€ reports
â”‚Â Â  â”œâ”€â”€ average_price_by_state_building-{date}.png
â”‚Â Â  â”œâ”€â”€ average_price_per_room-{date}.png
â”‚Â Â  â”œâ”€â”€ corelation_heatmap-{date}.png
â”‚Â Â  â”œâ”€â”€ frequency_construction_year-{date}.png
â”‚Â Â  â”œâ”€â”€ missing_bar-{date}.png
â”‚Â Â  â”œâ”€â”€ missing_pie-{date}.png
â”‚Â Â  â”œâ”€â”€ outlier_analysis-{date}.txt
â”‚Â Â  â”œâ”€â”€ prices-{date}.png
â”‚Â Â  â””â”€â”€ prices_boxplot-{date}.png
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ analysis_dashboard.py
â”‚Â Â  â”œâ”€â”€ api.py
â”‚Â Â  â”œâ”€â”€ data_cleaning_analysis.py
â”‚Â Â  â”œâ”€â”€ data_cleaning_training.py
â”‚Â Â  â”œâ”€â”€ data_scraping.py
â”‚Â Â  â”œâ”€â”€ training_models.py
â”‚Â Â  â””â”€â”€ web_ui.py
â”œâ”€â”€ utils
â”‚   â”œâ”€â”€ kitchen_ordinal.pkl
â”‚   â”œâ”€â”€ one_hot.pkl
â”‚   â”œâ”€â”€ state_building_ordinal.pkl
â”‚   â””â”€â”€ xgboost-{date}.pkl
â”œâ”€â”€ requiremts.txt
â””â”€â”€ README.md
```

## Our Pipeline
![image](https://github.com/user-attachments/assets/f1e64bd0-00a3-43f1-9bd8-8589f4d6aaf3)

## ğŸ›ï¸ How to use?

1. Clone the repo.
2. In your airflow virtual environment, install the requirements.
3. Run the airflow scheduler, the pipeline to scheduled to run at 2 am daily.
4. You can manually kick off the automation by running the DAG 'scrape_and_process_houses'.

## Sample from Airflow
![Screenshot 2024-12-23 100352](https://github.com/user-attachments/assets/2121237f-0091-45e8-a8fa-8c1c37c0386c)


## Enhancements
1. Train the model on the data scraped from the last 2 weeks instead of the last day.
2. Fix model deployment integration.
3. Build dashboard for the reports generated in streamlit.


## â±ï¸ Timeline

This project took five days for completion.

## ğŸ“Œ Personal Situation
This project was done as part of the AI Boocamp at BeCode.org. 


